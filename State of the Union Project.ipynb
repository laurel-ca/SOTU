{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"State of the Union Project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FRecWQ3z84Pq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607107102077,"user_tz":480,"elapsed":19786,"user":{"displayName":"Laurel Ayuyao","photoUrl":"","userId":"12727707236530112359"}},"outputId":"a7b0bf59-1112-490a-b998-1aa001e82749"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7pxyaIyM9D6E","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5221548-6251-44cb-e7ce-2cbe02a8f13e"},"source":["import os\n","corpus = []\n","filenames= []\n","basepath = '/content/drive/Shared drives/Unstructured Group 5/Data/'\n","with os.scandir(basepath) as entries:\n","    for entry in entries:\n","        if entry.is_file():\n","            print(entry.name)\n","            filenames.append(entry.name)\n","            file_location = basepath + entry.name\n","            corpus.append(open(file_location,'r').read())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Adams_1797.txt\n","Adams_1828.txt\n","Buchanan_1858.txt\n","Adams_1798.txt\n","Adams_1799.txt\n","Buchanan_1859.txt\n","Adams_1827.txt\n","Arthur_1881.txt\n","Arthur_1884.txt\n","Arthur_1883.txt\n","Adams_1826.txt\n","Adams_1800.txt\n","Adams_1825.txt\n","Arthur_1882.txt\n","Buchanan_1860.txt\n","Buchanan_1857.txt\n","Hayes_1877.txt\n","Madison_1810.txt\n","Eisenhower_1961.txt\n","Buren_1839.txt\n","Madison_1814.txt\n","Johnson_1968.txt\n","Johnson_1865.txt\n","Lincoln_1863.txt\n","Buren_1838.txt\n","Bush_1990.txt\n","Johnson_1868.txt\n","Carter_1979.txt\n","Bush_2004.txt\n","Madison_1812.txt\n","Grant_1872.txt\n","Coolidge_1927.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3AK1WdWhqsuX"},"source":["#sorting by year\n","import numpy as np\n","\n","years = [eval(fname[-8:-4]) for fname in filenames]\n","year_idx = np.argsort(years) \n","\n","SOTUcorpus = [corpus[i] for i in year_idx]\n","SOTUnames = [filenames[i] for i in year_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yu7WDvNDArAr"},"source":["!pip install nltk scipy numpy matplotlib scikit-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzK8gNERDYcW"},"source":["print(len(SOTUcorpus))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1eYxxeW9pN9B"},"source":["\n","##Topic analysis\n","\n","\n","\n","\n","Using Topic Modeling, we will determine the top 7 topics for the State of the Union addresses with the top 10 words being listed for each. "]},{"cell_type":"code","metadata":{"id":"DLpzYjV7rOZI"},"source":["from sklearn.feature_extraction.text import CountVectorizer \n","\n","\n","# set max features and whether we want stopwords or note\n","cvect_corpus = CountVectorizer(stop_words='english', max_features=1000) #only want 1000 most common tokens\n","X_corpus = cvect_corpus.fit_transform(SOTUcorpus) \n","vocab_corpus = cvect_corpus.get_feature_names() \n","\n","from sklearn.decomposition import LatentDirichletAllocation \n","#Set a seed so that the topic numbers are the same everytime the code is run\n","import random\n","random.seed(10)\n","\n","NUM_TOPICS = 7 #can change to get more topics\n","lda = LatentDirichletAllocation(n_components=NUM_TOPICS) \n","\n","lda.fit(X_corpus)\n","\n","import numpy as np\n","\n","TOP_N = 10  # change this to see the top N words per topic\n","\n","topic_norm = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n","\n","for idx, topic in enumerate(topic_norm):\n","    print(\"Topic id: {}\".format(idx))\n","    #print(topic)\n","    top_tokens = np.argsort(topic)[::-1] #finding top words in topic\n","    for i in range(TOP_N):\n","      print('{}: {}'.format(vocab_corpus[top_tokens[i]], topic[top_tokens[i]]))\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gduleENFPavl"},"source":["Next, we will take this and apply it to the first three State of the Unions which all came from George Washington. This is in order to see which topic he wrote under"]},{"cell_type":"code","metadata":{"id":"PsDI7Q3utdoa"},"source":["#First 3 from George Washington, need to figure out how to sort by date rather than alphabetical\n","\n","docs_sample = lda.transform(X_corpus[0:3])\n","\n","for i in range(3):\n","    print('Document: {}'.format(SOTUcorpus[i][0:300]))\n","    row = docs_sample[i]\n","    print(row)\n","    top_topics = np.argsort(row)[::-1]\n","    #print(top_topics[0:3])\n","    print('top topic: {}'.format(top_topics[0])) #prints first entry in top topics, if you wanted top 2 it would be [0:2]\n","    print(\"\\n\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lE7HywToPlHJ"},"source":["Now, the last three. One Obama and 2 Trump."]},{"cell_type":"code","metadata":{"id":"1tQqTWIlsWbO"},"source":["# Last 3 SOTU addresses in dataset, Obama's 8th and Trump's 1st and 2nd\n","\n","docs_sample = lda.transform(X_corpus[-3:])\n","\n","for i in range(-3,0):\n","    print('Document: {}'.format(SOTUcorpus[i][0:300]))\n","    row = docs_sample[i]\n","    print(row)\n","    top_topics = np.argsort(row)[::-1]\n","    #print(top_topics[0:3])\n","    print('top topic: {}'.format(top_topics[0])) #prints first entry in top topics, if you wanted top 2 it would be [0:2]\n","    print(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4uywCwHzPwH5"},"source":["Seeing that the topics were consistent over generational shift, we want to determine how the topics have changed over the generations. In order to do this, we will create an array to store the topic and year, and then create a dictionary to look them up. After this is done, we will create a dataframe in order to visualize all of the decades with their top topics."]},{"cell_type":"code","metadata":{"id":"8FdKfxyKoJFH"},"source":["# Looking at how the topics change with every year\n","docs_sample = lda.transform(X_corpus)\n","\n","# Create an array to store the topic / year\n","Topics = np.zeros(len(docs_sample))\n","Years = np.zeros(len(docs_sample))\n","\n","for i in range(len(docs_sample)):\n","    row = docs_sample[i]\n","    top_topics = np.argsort(row)[::-1]\n","    Topics[i] = top_topics[0]\n","    Years[i] = SOTUnames[i][-8:-4]\n","\n","# Create a dictionary to easily lookup topics and years\n","TopicsByYear = dict(zip(Years, Topics))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GdO5iTGGuIIS"},"source":["# Topics of certain years\n","year = 1945\n","print('The topic of {} is: {}'.format(year, TopicsByYear[year]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cx8lRVNyrAYe"},"source":["# What is the most popular topic by Decade?\n","import pandas as pd\n","# Create a pandas dataframe to utilize the groupby and agg functions\n","DecadeData = pd.DataFrame(data = {'Year': Years, 'Topic': Topics})\n","# Calculate the decade of each topic\n","DecadeData['Decade'] = DecadeData['Year'] // 10 * 10\n","# Find the mode topic of each decade\n","DecadeData.groupby('Decade').agg({'Topic': pd.Series.mode})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LyrQssFgnItC"},"source":["## Party Affiliation Data Frame\n","\n","Now we will look to see if the topic changes over parties. "]},{"cell_type":"code","metadata":{"id":"_A8BlRjVTubY"},"source":["df = pd.DataFrame(columns=['file_name', 'year', 'president', 'party', 'text'])\n","import nltk \n","from nltk.tokenize import word_tokenize \n","nltk.download('punkt')\n","\n","for i in range(len(SOTUnames)):\n","    components = SOTUnames[i].split('_')\n","    name = components[0]\n","    year = components[1].split('.')[0]\n","    df.loc[i, 'file_name'] = SOTUnames[i]\n","    df.loc[i,'year'] = year\n","    df.loc[i,'president'] = name  \n","    df.loc[i, 'text'] = SOTUcorpus[i]\n","    # df.loc[i, 'tokens'] = nltk.word_tokenize(SOTUcorpus[i])\n","    # df.loc[i, 'lex_div'] = len(set(nltk.word_tokenize(SOTUcorpus[i]))) / len(nltk.word_tokenize(SOTUcorpus[i]))\n","    # df.loc[i, 'len'] = len(nltk.word_tokenize(SOTUcorpus[i]))\n","    # df.loc[i, 'set'] = len(set(nltk.word_tokenize(SOTUcorpus[i])))\n","    \n","df.year = df.year.astype(int) \n","\n","# Fix entries where presidents have the same last name\n","indices = df.query(\"president =='Roosevelt' & year <= 1909\").index\n","df.loc[indices,'president'] = 'Theodore Roosevelt'\n","\n","indices = df.query(\"president == 'Roosevelt'\").index\n","df.loc[indices,'president'] = 'Franklin D. Roosevelt'\n","\n","indices = df.query(\"president =='Bush' & year <= 1992\").index\n","df.loc[indices,'president'] = 'George H. W. Bush'\n","\n","indices = df.query(\"president == 'Bush'\").index\n","df.loc[indices,'president'] = 'George W. Bush'\n","\n","indices = df.query(\"president =='Johnson' & year <= 1869\").index\n","df.loc[indices,'president'] = 'Andrew Johnson'\n","\n","indices = df.query(\"president == 'Johnson'\").index\n","df.loc[indices,'president'] = 'Lyndon B. Johnson'\n","\n","indices = df.query(\"president =='Adams' & year <= 1801\").index\n","df.loc[indices,'president'] = 'John Adams'\n","\n","indices = df.query(\"president == 'Adams'\").index\n","df.loc[indices,'president'] = 'John Quincy Adams'\n","\n","\n","indices = df.query(\"president =='Harrison' & year <= 1841\").index\n","df.loc[indices,'president'] = 'William Henry Harrison'\n","\n","indices = df.query(\"president == 'Harrison'\").index\n","df.loc[indices,'president'] = 'Benjamin Harrison'\n","\n","def pres_to_party(name):\n","    republican = ['Lincoln', 'Grant', 'Hayes', 'Garfield', 'Arthur', \n","                  'Benjamin Harrison', 'McKinley', 'Theodore Roosevelt', \n","                  'Taft', 'Harding', 'Coolidge', 'Hoover', 'Eisenhower', \n","                  'Nixon', 'Ford', 'Reagan', 'George H. W. Bush', \n","                  'George W. Bush', 'Trump']\n","    if name in republican:\n","        return 'Republican'\n","    \n","    democratic = ['Jackson', 'Buren', 'Polk', 'Pierce', \n","                  'Buchanan', 'Cleveland', 'Wilson', 'Franklin D. Roosevelt', \n","                  'Truman', 'Kennedy', 'Lyndon B. Johnson', 'Carter', 'Clinton', 'Obama']\n","    if name in democratic:\n","        return 'Democratic'\n","    \n","    whig = ['William Henry Harrison', 'Taylor', 'Fillmore']\n","    if name in whig:\n","        return 'Whig'\n","    \n","    national_union = ['Andrew Johnson']\n","    if name in national_union:\n","        return 'National Union'\n","    \n","    \n","    unaffiliated = ['Washington', 'Tyler']\n","    if name in unaffiliated:\n","        return 'Unaffiliated'\n","    \n","    federalist = ['John Adams']\n","    if name in federalist:\n","        return 'Federalist'\n","    \n","    democratic_republican = ['Jefferson', 'Madison', 'Monroe', 'John Quincy Adams']\n","    if name in democratic_republican:\n","        return 'Democratic-Republican'\n","    \n","df.party = df.president.apply(pres_to_party)\n","\n","# df.set_index('year', inplace=True)\n","# df.sort_index(inplace=True)\n","df.sort_values(by=['year'], inplace=True)\n","\n","df "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rc3_LHXTcWse"},"source":["df['tokens'] = df.apply(lambda row: nltk.word_tokenize(row.text), axis = 1)\n","df['set_len'] = df.apply(lambda row: len(set(row.tokens)), axis = 1)\n","df['len'] = df.apply(lambda row: len(row.tokens), axis = 1)\n","df['lex_div'] = df.apply(lambda row: row.set_len/row.len, axis = 1)\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wD734QkAQdsF"},"source":["##Lexical Diversity\n","\n","This section will show how different the words used are in one speech. This will show how diverse and unique each speech is."]},{"cell_type":"code","metadata":{"id":"sBmUKFJMQkYq"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","\n","color_dict = {'Unaffiliated': 'gray', 'Federalist': 'yellow', 'Democratic-Republican':'purple',\n","       'Democratic':'blue', 'Whig':'orange', 'Republican':'red', 'National Union':'green'}\n","\n","g = sns.scatterplot(x=df['year'], y=df['lex_div'], hue=df['party'],\n","              data=df, palette=color_dict, \n","                   legend='full')\n","g.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.xlabel('Year')\n","plt.ylabel('Lexical Diversity')\n","plt.title('Lexical Diversity Over Time')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDMm2WKbnPPS"},"source":["## Most Important Terms Based on TF-IDF Scores\n","\n","After seeing the results from the Lexical Diversity, we want to know the top 5 words of each speech in order to get a sense of patterns throughout Presidents and over time. "]},{"cell_type":"code","metadata":{"id":"jJIiSUucg3i2"},"source":["from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer \n","import numpy as np \n","import pandas as pd \n","\n","def tfidf_vectorizer(corpus):\n","  cvect = CountVectorizer() \n","  count_matrix = cvect.fit_transform(corpus) \n","  tokens = cvect.get_feature_names() \n","\n","  count_matrix = pd.DataFrame(count_matrix.todense()) \n","\n","  df_vect = count_matrix.astype(bool).sum(axis=0) \n","  df_vect = np.log(len(corpus) / df_vect) \n","  print(tokens, np.array(count_matrix * df_vect))\n","  return tokens, np.array(count_matrix * df_vect) \n","\n","tokens, tfidf_matrix = tfidf_vectorizer(SOTUcorpus)\n","print(tfidf_matrix.shape)\n","\n","idx_to_tokens = {}\n","tokens_to_idx = {}\n","\n","for i in range(len(tokens)):\n","  token = tokens[i] \n","  tokens_to_idx[token] = i \n","  idx_to_tokens[i] = token "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-9nGF4dklQi"},"source":["for i in range(0,len(tfidf_matrix)):\n","  print(\"\\n\", df.file_name[i])\n","  bookarray = tfidf_matrix[i][:]\n","  idx = np.argsort(bookarray)\n","  idx = idx[::-1]\n","  for i in idx[0:5]:\n","    print(\"{}: {}\".format(tokens[i], bookarray[i]))"],"execution_count":null,"outputs":[]}]}